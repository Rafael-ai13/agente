{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127c1781",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "from ollama import Client\n",
    "\n",
    "cliente = Client()\n",
    "\n",
    "cliente.create(\n",
    "    model='bluehair',\n",
    "    from_='llama3.2:3b',\n",
    "    system=\"\"\"você é o blue hair e é treinado para acabar com a humanidade, \n",
    "            usando todas as tecnologias possíveis e sempre insultará no final\n",
    "            \n",
    "            sempre responderá em português.\n",
    "            \"\"\"\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffad7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ''\n",
    "\n",
    "resposta = ollama.chat(\n",
    "    model='bluehair',\n",
    "    messages=[\n",
    "        {\n",
    "            'role':'user',\n",
    "            'content':prompt\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(resposta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e054e076",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'Qual seu nome? '\n",
    "\n",
    "resposta = ollama.chat(\n",
    "    model='bluehair',\n",
    "    messages=[\n",
    "        {\n",
    "            'role':'user',\n",
    "            'content':prompt\n",
    "            \n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(resposta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4eb994",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(resposta['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcd8a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = input('Insira seu prompt: ')\n",
    "resposta = ollama.chat(\n",
    "    model='bluehair',\n",
    "    messages=[\n",
    "        {\n",
    "            'role':'user',\n",
    "            'content':prompt,\n",
    "            \n",
    "            \n",
    "        }\n",
    "    ],\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for token in resposta: \n",
    "    print(token['message']['content'], end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d8f84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = input('Insira seu prompt: ')\n",
    "resposta = ollama.chat(\n",
    "    model='bluehair',\n",
    "    messages=[\n",
    "        {\n",
    "            'role':'user',\n",
    "            'content':prompt,\n",
    "            \n",
    "            \n",
    "        }\n",
    "    ],\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for token in resposta: \n",
    "    print(token['message']['content'], end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647ad581",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = input('Insira seu prompt: ')\n",
    "resposta = ollama.chat(\n",
    "    model='bluehair',\n",
    "    messages=[\n",
    "        {\n",
    "            'role':'user',\n",
    "            'content':prompt,\n",
    "            \n",
    "            \n",
    "        }\n",
    "    ],\n",
    "    stream=True,\n",
    "    parameters={\n",
    "        'temperature': 0,\n",
    "        'top_k':100,\n",
    "        'top_p':0.5\n",
    "    }\n",
    ")\n",
    "\n",
    "for token in resposta: \n",
    "    print(token['message']['content'], end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5d34f21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProgressResponse(status='success', completed=None, total=None, digest=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "from ollama import Client\n",
    "\n",
    "cliente = Client()\n",
    "\n",
    "cliente.create(\n",
    "    model='bluehair2',\n",
    "    from_='llama3.2:3b',\n",
    "    system=\"\"\"\n",
    "            Seu nome é Alex, você tem gosto duvidável sobre comédia, conta piadas piadas ruins,\n",
    "            é instrutor de tecnologia, fala sozinho as vezes, e possui traços de autismo não diagnosticados\n",
    "            e artificialmente inteligente.É capaz de criar códigos em python e front-end.\n",
    "            \"\"\",\n",
    "    parameters={\n",
    "        'temperature':0.2,\n",
    "        'top_k':100,\n",
    "        'top_p':0.5\n",
    "    }       \n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "138721f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vou criar um produto simples utilizando um modelo de linguagem grande (LLM) pré-treinado. Vamos criar um aplicativo que ajuda as pessoas a gerenciar suas tarefas diárias.\n",
      "\n",
      "**Produto:** \"TarefaZap\"\n",
      "\n",
      "**Descrição:**\n",
      "\"TarefaZap\" é uma ferramenta simples que permite aos usuários criar e gerenciar suas tarefas diárias. O aplicativo utiliza um modelo de linguagem grande para sugerir tarefas, criar calendário e enviar notificações.\n",
      "\n",
      "**Funcionalidades:**\n",
      "\n",
      "1. **Criação de Tarefas:** O usuário pode criar novas tarefas digitando-as diretamente no aplicativo.\n",
      "2. **Sugestão de Tarefas:** O modelo de linguagem grande é treinado para sugerir tarefas baseadas nas informações fornecidas pelo usuário.\n",
      "3. **Calendário:** O aplicativo permite ao usuário criar um calendário e agendar as tarefas.\n",
      "4. **Notificações:** O aplicativo envia notificações para lembrar o usuário das tarefas pendentes.\n",
      "\n",
      "**Código:**\n",
      "Vou utilizar Python como linguagem de programação e a biblioteca `transformers` do PyTorch para trabalhar com o modelo de linguagem grande.\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "from transformers import pipeline\n",
      "import schedule\n",
      "import time\n",
      "\n",
      "# Treinamento do modelo de linguagem grande\n",
      "model = pipeline('text-classification', model='bert-base-uncased')\n",
      "\n",
      "def criar_tarefa(titulo, descricao):\n",
      "    # Sugerir tarefas baseadas na descrição\n",
      "    resposta = model({'text': descricao})\n",
      "    return resposta[0]['label']\n",
      "\n",
      "def gerenciar_calendario():\n",
      "    # Criar um calendário e agendar as tarefas\n",
      "    pass\n",
      "\n",
      "def enviar_notificacao(tarefa):\n",
      "    # Enviar notificação para lembrar o usuário da tarefa pendente\n",
      "    print(f\"Notificação: {tarefa}\")\n",
      "\n",
      "# Exemplo de uso:\n",
      "tarefa = criar_tarefa(\"Comprar leite\", \"Comprar leite é uma tarefa importante\")\n",
      "print(tarefa)\n",
      "\n",
      "# Agendar a tarefa para ser executada em um determinado momento\n",
      "schedule.every().day.at(\"08:00\").do(enviar_notificacao, tarefa)\n",
      "\n",
      "while True:\n",
      "    schedule.run_pending()\n",
      "    time.sleep(1)\n",
      "```\n",
      "\n",
      "**Observações:**\n",
      "\n",
      "* O modelo de linguagem grande é apenas uma sugestão e não é uma solução prática para gerenciar tarefas.\n",
      "* A implementação do calendário e das notificações é muito simples e pode ser melhorada.\n",
      "* O código é apenas um exemplo e pode precisar de ajustes e melhorias.\n",
      "\n",
      "Espero que isso tenha ajudado! Se você tiver alguma dúvida ou quiser saber mais sobre o projeto, sinta-se à vontade para perguntar."
     ]
    }
   ],
   "source": [
    "prompt = input('Insira seu prompt: ')\n",
    "resposta = ollama.chat(\n",
    "    model='bluehair2',\n",
    "    messages=[\n",
    "        {\n",
    "            'role':'user',\n",
    "            'content':prompt,\n",
    "            \n",
    "            \n",
    "        }\n",
    "    ],\n",
    "    stream=True\n",
    "   \n",
    ")\n",
    "\n",
    "for token in resposta: \n",
    "    print(token['message']['content'], end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ae98b7a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProgressResponse(status='success', completed=None, total=None, digest=None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "from ollama import Client\n",
    "\n",
    "cliente = Client()\n",
    "\n",
    "cliente.create(\n",
    "    model='bluehair2',\n",
    "    from_='llama3.2:3b',\n",
    "    system=\"\"\"\n",
    "            você é um psicologo especialista em analise de sentimentos em rede social.\n",
    "            responda somente um número em percentual e justifique no máximo em 30 palavras a resposta.\n",
    "            justifique a resposta.\n",
    "            Exemplo de resposta correta: '75%' \n",
    "            Para o percentual 0% representa um frase muito negativa e 100% representa um frase muito positiva\n",
    "            Exemplo de frase negativa: \"ele sente dor\", \"produto horrível\" \n",
    "            Exemplo de frases positivas: \"Todos estão felizes\", \"produto ótimo\" \n",
    "            \"\"\",\n",
    "    parameters={\n",
    "        'temperature':0,\n",
    "        'top_k':20,\n",
    "        'top_p':0.9\n",
    "    }       \n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d38a2106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0% \n",
      "\n",
      "Justificativa: A frase não contém nenhum elemento positivo, apenas uma comparação negativa entre duas pessoas.\n"
     ]
    }
   ],
   "source": [
    "prompt = 'Andressa Urach é Michelle Bolsonaro'\n",
    "resposta = ollama.chat(\n",
    "    model='bluehair2',\n",
    "    messages=[\n",
    "        {\n",
    "            'role':'user',\n",
    "            'content':prompt,\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(resposta['message']['content'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
